{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Translation\n",
    "In this project, I'll be training a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French.\n",
    "## The Data\n",
    "Since translating the whole language of English to French will take lots of time to train, I've used only a small portion of the English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Played around with view_sentence_range to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 227\n",
      "Number of sentences: 137861\n",
      "Average number of words in a sentence: 13.225277634719028\n",
      "\n",
      "English sentences 0 to 10:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "\n",
      "French sentences 0 to 10:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Preprocessing Function\n",
    "### Text to Word Ids\n",
    "First I need to turn the text into a number so the computer can understand it. In the function `text_to_ids()`, I'll convert `source_text` and `target_text` from words to ids.  I've also added the `<EOS>` word id at the end of `target_text`.  This will help the neural network predict when the sentence should end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    source_id_text = [[source_vocab_to_int[word] for word in sentence.split()] for sentence in source_text.split('\\n')]\n",
    "        \n",
    "    target_id_text = [[target_vocab_to_int[word] for word in sentence.split()] + [target_vocab_to_int['<EOS>']]\n",
    "              for sentence in target_text.split('\\n')]\n",
    "\n",
    "    return source_id_text, target_id_text\n",
    "\n",
    "tests.test_text_to_ids(text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed all the data and now saving it~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Checking TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Checking for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network\n",
    "I've build the components necessary to build a Sequence-to-Sequence model by implementing the following functions below:\n",
    "- `model_inputs`\n",
    "- `process_decoder_input`\n",
    "- `encoding_layer`\n",
    "- `decoding_layer_train`\n",
    "- `decoding_layer_infer`\n",
    "- `decoding_layer`\n",
    "- `seq2seq_model`\n",
    "\n",
    "### Input\n",
    "The `model_inputs()` function is used to create the following TF Placeholders for the Neural Network:\n",
    "\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2.\n",
    "- Targets placeholder with rank 2.\n",
    "- Learning rate placeholder with rank 0.\n",
    "- Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0.\n",
    "- Target sequence length placeholder named \"target_sequence_length\" with rank 1\n",
    "- Max target sequence length tensor named \"max_target_len\" getting its value from applying tf.reduce_max on the target_sequence_length placeholder. Rank 0.\n",
    "- Source sequence length placeholder named \"source_sequence_length\" with rank 1\n",
    "\n",
    "The placeholders are returned in the following the tuple (input, targets, learning rate, keep probability, target sequence length, max target sequence length, source sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def model_inputs():\n",
    " \n",
    "    # TODO: Implement Function\n",
    "    Input = tf.placeholder(tf.int32, [None, None], name= 'input')\n",
    "    target = tf.placeholder(tf.int32, [None, None], name= 'target')\n",
    "    lr = tf.placeholder(tf.float32,name='lr')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    target_sequence_length = tf.placeholder(tf.int32,[None,],name ='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)\n",
    "    source_sequence_length = tf.placeholder(tf.int32,[None,],name ='source_sequence_length')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (Input, target, lr, keep_prob, target_sequence_length,max_target_len, source_sequence_length)\n",
    "\n",
    "\n",
    "tests.test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoder Input\n",
    "The function `process_decoder_input` is implemented below by removing the last word id from each batch in `target_data` and concating the GO ID to the begining of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "    return dec_input\n",
    "    \n",
    "\n",
    "\n",
    "tests.test_process_encoding_input(process_decoder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Below are the \"To-Dos\" for the  `encoding_layer()` function to create a Encoder RNN layer:\n",
    " * Embed the encoder input using [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence)\n",
    " * Construct a [stacked](https://github.com/tensorflow/tensorflow/blob/6947f65a374ebf29e74bb71e36fd82760056d82c/tensorflow/docs_src/tutorials/recurrent.md#stacking-multiple-lstms) [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) wrapped in a [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper)\n",
    " * Pass cell and embedded input to [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "reload(tests)\n",
    "\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers,\n",
    "                   keep_prob, source_sequence_length,\n",
    "                   source_vocab_size,encoding_embedding_size): \n",
    "                                   \n",
    "  \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(rnn_inputs, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "\n",
    "    def make_cell(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "                                    \n",
    "        return enc_cell\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state\n",
    "\n",
    "tests.test_encoding_layer(encoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "To-Dos for creating a training decoding layer:\n",
    "* Create a [`tf.contrib.seq2seq.TrainingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper) \n",
    "* Create a [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n",
    "* Obtain the decoder outputs from [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "   \n",
    "    # TODO: Just Implement Function\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    " \n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, encoder_state, output_layer)\n",
    "    \n",
    "    BasicDecoderOutput,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder, impute_finished=True, maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return BasicDecoderOutput\n",
    "\n",
    "\n",
    "tests.test_decoding_layer_train(decoding_layer_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference\n",
    "To-Dos for Creating inference decoder:\n",
    "* Create a [`tf.contrib.seq2seq.GreedyEmbeddingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper)\n",
    "* Create a [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n",
    "* Obtain the decoder outputs from [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                                start_tokens=tf.tile([start_of_sequence_id], [batch_size]),\n",
    "                                                                end_token=end_of_sequence_id)\n",
    "\n",
    "    \n",
    "    \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       inference_helper,\n",
    "                                                       encoder_state,\n",
    "                                                       output_layer)\n",
    "    \n",
    "    \n",
    "       \n",
    "    \n",
    "    inference_decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(decoder=inference_decoder,\n",
    "                                                                impute_finished=True,\n",
    "                                                                maximum_iterations=max_target_sequence_length)\n",
    "\n",
    "\n",
    "    \n",
    "    return inference_decoder_output\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "tests.test_decoding_layer_infer(decoding_layer_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Decoding Layer\n",
    "Next `decoding_layer()` function is implemented to create a Decoder RNN layer.\n",
    "The To-Dos~~ LOL\n",
    "* Embed the target sequences\n",
    "* Construct the decoder LSTM cell (just like we constructed the encoder cell above)\n",
    "* Create an output layer to map the outputs of the decoder to the elements of our vocabulary\n",
    "* Use the `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, target_sequence_length, max_target_sequence_length, output_layer, keep_prob)` function to get the training logits.\n",
    "* Use the `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, max_target_sequence_length, vocab_size, output_layer, batch_size, keep_prob)` function to get the inference logits.\n",
    "\n",
    "Note: I've used [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to share variables between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    def build_cell(rnn_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "        lstm_drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return lstm_drop\n",
    "    # Stack them all\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    dense_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    with tf.variable_scope(\"decode\") as scope:\n",
    "        tr_decoder_output = decoding_layer_train(\n",
    "            encoder_state, stacked_lstm, dec_embed_input, \n",
    "            target_sequence_length, max_target_sequence_length, \n",
    "            dense_layer, keep_prob)\n",
    "        scope.reuse_variables()\n",
    "        inf_decoder_output = decoding_layer_infer(\n",
    "            encoder_state, stacked_lstm, dec_embeddings, \n",
    "            target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], \n",
    "            max_target_sequence_length, target_vocab_size, \n",
    "            dense_layer, batch_size, keep_prob)\n",
    "    \n",
    "    return tr_decoder_output, inf_decoder_output\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "tests.test_decoding_layer(decoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Neural Network\n",
    "Applying the above functions to~~\n",
    "\n",
    "- Apply embedding to the input data for the encoder.\n",
    "- Encode the input using the `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob,  source_sequence_length, source_vocab_size, encoding_embedding_size)`.\n",
    "- Process target data using the `process_decoder_input(target_data, target_vocab_to_int, batch_size)` function.\n",
    "- Apply embedding to the target data for the decoder.\n",
    "- Decode the encoded input using the `decoding_layer(dec_input, enc_state, target_sequence_length, max_target_sentence_length, rnn_size, num_layers, target_vocab_to_int, target_vocab_size, batch_size, keep_prob, dec_embedding_size)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "   \n",
    "    # TODO: Implement Function\n",
    "    _,enc_state = encoding_layer(input_data, rnn_size,\n",
    "                                     num_layers, keep_prob,\n",
    "                                     source_sequence_length,\n",
    "                                     source_vocab_size, enc_embedding_size)\n",
    "    \n",
    "    \n",
    "    # Prepare the target sequences we'll feed to the decoder in training mode\n",
    "    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    # Pass encoder state and decoder inputs to the decoders\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(dec_input, enc_state, \n",
    "                                                                       target_sequence_length, \n",
    "                                                                       max_target_sentence_length, rnn_size, \n",
    "                                                                       num_layers, target_vocab_to_int, \n",
    "                                                                       target_vocab_size, batch_size, keep_prob,\n",
    "                                                                       dec_embedding_size)\n",
    "    \n",
    "    return (training_decoder_output, inference_decoder_output)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "tests.test_seq2seq_model(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tuning the parameters~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 4\n",
    "# Batch Size\n",
    "batch_size =128\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 150\n",
    "decoding_embedding_size = 150\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.8\n",
    "display_step = 10   #to state how many steps between each debug output statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()\n",
    "\n",
    "    #sequence_length = tf.placeholder_with_default(max_target_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch and pad the source and target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Training the neural network below on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   10/1077 - Train Accuracy: 0.2858, Validation Accuracy: 0.3803, Loss: 3.6743\n",
      "Epoch   0 Batch   20/1077 - Train Accuracy: 0.3852, Validation Accuracy: 0.4482, Loss: 2.8504\n",
      "Epoch   0 Batch   30/1077 - Train Accuracy: 0.3910, Validation Accuracy: 0.4627, Loss: 2.7234\n",
      "Epoch   0 Batch   40/1077 - Train Accuracy: 0.4223, Validation Accuracy: 0.4883, Loss: 2.5103\n",
      "Epoch   0 Batch   50/1077 - Train Accuracy: 0.4219, Validation Accuracy: 0.4950, Loss: 2.4129\n",
      "Epoch   0 Batch   60/1077 - Train Accuracy: 0.4721, Validation Accuracy: 0.5160, Loss: 2.1428\n",
      "Epoch   0 Batch   70/1077 - Train Accuracy: 0.4149, Validation Accuracy: 0.4794, Loss: 2.1404\n",
      "Epoch   0 Batch   80/1077 - Train Accuracy: 0.4434, Validation Accuracy: 0.5053, Loss: 1.9330\n",
      "Epoch   0 Batch   90/1077 - Train Accuracy: 0.4609, Validation Accuracy: 0.5188, Loss: 1.8752\n",
      "Epoch   0 Batch  100/1077 - Train Accuracy: 0.4770, Validation Accuracy: 0.5192, Loss: 1.7621\n",
      "Epoch   0 Batch  110/1077 - Train Accuracy: 0.4762, Validation Accuracy: 0.4968, Loss: 1.5953\n",
      "Epoch   0 Batch  120/1077 - Train Accuracy: 0.4656, Validation Accuracy: 0.5025, Loss: 1.5525\n",
      "Epoch   0 Batch  130/1077 - Train Accuracy: 0.4576, Validation Accuracy: 0.4954, Loss: 1.3998\n",
      "Epoch   0 Batch  140/1077 - Train Accuracy: 0.4268, Validation Accuracy: 0.5050, Loss: 1.5175\n",
      "Epoch   0 Batch  150/1077 - Train Accuracy: 0.4825, Validation Accuracy: 0.5103, Loss: 1.3101\n",
      "Epoch   0 Batch  160/1077 - Train Accuracy: 0.4371, Validation Accuracy: 0.4794, Loss: 1.3079\n",
      "Epoch   0 Batch  170/1077 - Train Accuracy: 0.4813, Validation Accuracy: 0.5156, Loss: 1.3525\n",
      "Epoch   0 Batch  180/1077 - Train Accuracy: 0.4996, Validation Accuracy: 0.5440, Loss: 1.2277\n",
      "Epoch   0 Batch  190/1077 - Train Accuracy: 0.4813, Validation Accuracy: 0.5121, Loss: 1.1631\n",
      "Epoch   0 Batch  200/1077 - Train Accuracy: 0.4852, Validation Accuracy: 0.5440, Loss: 1.1336\n",
      "Epoch   0 Batch  210/1077 - Train Accuracy: 0.5219, Validation Accuracy: 0.5629, Loss: 1.0930\n",
      "Epoch   0 Batch  220/1077 - Train Accuracy: 0.5169, Validation Accuracy: 0.5682, Loss: 1.0618\n",
      "Epoch   0 Batch  230/1077 - Train Accuracy: 0.5513, Validation Accuracy: 0.5749, Loss: 0.9580\n",
      "Epoch   0 Batch  240/1077 - Train Accuracy: 0.5371, Validation Accuracy: 0.5611, Loss: 0.9644\n",
      "Epoch   0 Batch  250/1077 - Train Accuracy: 0.5366, Validation Accuracy: 0.5593, Loss: 0.8532\n",
      "Epoch   0 Batch  260/1077 - Train Accuracy: 0.5335, Validation Accuracy: 0.5632, Loss: 0.8549\n",
      "Epoch   0 Batch  270/1077 - Train Accuracy: 0.4934, Validation Accuracy: 0.5700, Loss: 0.9091\n",
      "Epoch   0 Batch  280/1077 - Train Accuracy: 0.5602, Validation Accuracy: 0.5739, Loss: 0.8694\n",
      "Epoch   0 Batch  290/1077 - Train Accuracy: 0.5266, Validation Accuracy: 0.5753, Loss: 0.8482\n",
      "Epoch   0 Batch  300/1077 - Train Accuracy: 0.5185, Validation Accuracy: 0.5998, Loss: 0.8222\n",
      "Epoch   0 Batch  310/1077 - Train Accuracy: 0.5355, Validation Accuracy: 0.5877, Loss: 0.7921\n",
      "Epoch   0 Batch  320/1077 - Train Accuracy: 0.5797, Validation Accuracy: 0.6033, Loss: 0.7657\n",
      "Epoch   0 Batch  330/1077 - Train Accuracy: 0.6113, Validation Accuracy: 0.5884, Loss: 0.7303\n",
      "Epoch   0 Batch  340/1077 - Train Accuracy: 0.5625, Validation Accuracy: 0.6151, Loss: 0.7321\n",
      "Epoch   0 Batch  350/1077 - Train Accuracy: 0.5109, Validation Accuracy: 0.5906, Loss: 0.7455\n",
      "Epoch   0 Batch  360/1077 - Train Accuracy: 0.5996, Validation Accuracy: 0.6271, Loss: 0.6857\n",
      "Epoch   0 Batch  370/1077 - Train Accuracy: 0.5785, Validation Accuracy: 0.6268, Loss: 0.6331\n",
      "Epoch   0 Batch  380/1077 - Train Accuracy: 0.6070, Validation Accuracy: 0.6278, Loss: 0.6430\n",
      "Epoch   0 Batch  390/1077 - Train Accuracy: 0.5852, Validation Accuracy: 0.6293, Loss: 0.6618\n",
      "Epoch   0 Batch  400/1077 - Train Accuracy: 0.6227, Validation Accuracy: 0.6293, Loss: 0.6456\n",
      "Epoch   0 Batch  410/1077 - Train Accuracy: 0.5913, Validation Accuracy: 0.6413, Loss: 0.6439\n",
      "Epoch   0 Batch  420/1077 - Train Accuracy: 0.6074, Validation Accuracy: 0.6417, Loss: 0.5951\n",
      "Epoch   0 Batch  430/1077 - Train Accuracy: 0.6047, Validation Accuracy: 0.6186, Loss: 0.5976\n",
      "Epoch   0 Batch  440/1077 - Train Accuracy: 0.6469, Validation Accuracy: 0.6388, Loss: 0.6148\n",
      "Epoch   0 Batch  450/1077 - Train Accuracy: 0.5977, Validation Accuracy: 0.6378, Loss: 0.5562\n",
      "Epoch   0 Batch  460/1077 - Train Accuracy: 0.6168, Validation Accuracy: 0.6396, Loss: 0.5710\n",
      "Epoch   0 Batch  470/1077 - Train Accuracy: 0.5987, Validation Accuracy: 0.6456, Loss: 0.5807\n",
      "Epoch   0 Batch  480/1077 - Train Accuracy: 0.6595, Validation Accuracy: 0.6403, Loss: 0.5546\n",
      "Epoch   0 Batch  490/1077 - Train Accuracy: 0.6398, Validation Accuracy: 0.6353, Loss: 0.5499\n",
      "Epoch   0 Batch  500/1077 - Train Accuracy: 0.6520, Validation Accuracy: 0.6598, Loss: 0.5182\n",
      "Epoch   0 Batch  510/1077 - Train Accuracy: 0.6551, Validation Accuracy: 0.6357, Loss: 0.4991\n",
      "Epoch   0 Batch  520/1077 - Train Accuracy: 0.6946, Validation Accuracy: 0.6783, Loss: 0.4699\n",
      "Epoch   0 Batch  530/1077 - Train Accuracy: 0.6730, Validation Accuracy: 0.6662, Loss: 0.4868\n",
      "Epoch   0 Batch  540/1077 - Train Accuracy: 0.7109, Validation Accuracy: 0.6907, Loss: 0.4413\n",
      "Epoch   0 Batch  550/1077 - Train Accuracy: 0.6973, Validation Accuracy: 0.6925, Loss: 0.4812\n",
      "Epoch   0 Batch  560/1077 - Train Accuracy: 0.6844, Validation Accuracy: 0.6562, Loss: 0.4471\n",
      "Epoch   0 Batch  570/1077 - Train Accuracy: 0.7052, Validation Accuracy: 0.6942, Loss: 0.4719\n",
      "Epoch   0 Batch  580/1077 - Train Accuracy: 0.7195, Validation Accuracy: 0.6765, Loss: 0.3977\n",
      "Epoch   0 Batch  590/1077 - Train Accuracy: 0.6809, Validation Accuracy: 0.7088, Loss: 0.4667\n",
      "Epoch   0 Batch  600/1077 - Train Accuracy: 0.7195, Validation Accuracy: 0.7248, Loss: 0.4042\n",
      "Epoch   0 Batch  610/1077 - Train Accuracy: 0.7007, Validation Accuracy: 0.6971, Loss: 0.4200\n",
      "Epoch   0 Batch  620/1077 - Train Accuracy: 0.7422, Validation Accuracy: 0.7489, Loss: 0.3781\n",
      "Epoch   0 Batch  630/1077 - Train Accuracy: 0.7477, Validation Accuracy: 0.7635, Loss: 0.3837\n",
      "Epoch   0 Batch  640/1077 - Train Accuracy: 0.7452, Validation Accuracy: 0.7528, Loss: 0.3604\n",
      "Epoch   0 Batch  650/1077 - Train Accuracy: 0.7227, Validation Accuracy: 0.7724, Loss: 0.3672\n",
      "Epoch   0 Batch  660/1077 - Train Accuracy: 0.7547, Validation Accuracy: 0.7688, Loss: 0.3687\n",
      "Epoch   0 Batch  670/1077 - Train Accuracy: 0.7947, Validation Accuracy: 0.7479, Loss: 0.3365\n",
      "Epoch   0 Batch  680/1077 - Train Accuracy: 0.7574, Validation Accuracy: 0.7852, Loss: 0.3273\n",
      "Epoch   0 Batch  690/1077 - Train Accuracy: 0.7914, Validation Accuracy: 0.7884, Loss: 0.3217\n",
      "Epoch   0 Batch  700/1077 - Train Accuracy: 0.7988, Validation Accuracy: 0.7876, Loss: 0.2951\n",
      "Epoch   0 Batch  710/1077 - Train Accuracy: 0.7949, Validation Accuracy: 0.7834, Loss: 0.3016\n",
      "Epoch   0 Batch  720/1077 - Train Accuracy: 0.7677, Validation Accuracy: 0.7667, Loss: 0.3214\n",
      "Epoch   0 Batch  730/1077 - Train Accuracy: 0.8207, Validation Accuracy: 0.7855, Loss: 0.2875\n",
      "Epoch   0 Batch  740/1077 - Train Accuracy: 0.7797, Validation Accuracy: 0.7933, Loss: 0.2771\n",
      "Epoch   0 Batch  750/1077 - Train Accuracy: 0.7828, Validation Accuracy: 0.7805, Loss: 0.2742\n",
      "Epoch   0 Batch  760/1077 - Train Accuracy: 0.7973, Validation Accuracy: 0.8001, Loss: 0.2913\n",
      "Epoch   0 Batch  770/1077 - Train Accuracy: 0.8203, Validation Accuracy: 0.8263, Loss: 0.2486\n",
      "Epoch   0 Batch  780/1077 - Train Accuracy: 0.7898, Validation Accuracy: 0.8132, Loss: 0.2753\n",
      "Epoch   0 Batch  790/1077 - Train Accuracy: 0.7605, Validation Accuracy: 0.8189, Loss: 0.2628\n",
      "Epoch   0 Batch  800/1077 - Train Accuracy: 0.8469, Validation Accuracy: 0.8214, Loss: 0.2391\n",
      "Epoch   0 Batch  810/1077 - Train Accuracy: 0.8620, Validation Accuracy: 0.8104, Loss: 0.2018\n",
      "Epoch   0 Batch  820/1077 - Train Accuracy: 0.8242, Validation Accuracy: 0.8065, Loss: 0.2160\n",
      "Epoch   0 Batch  830/1077 - Train Accuracy: 0.8137, Validation Accuracy: 0.8281, Loss: 0.2092\n",
      "Epoch   0 Batch  840/1077 - Train Accuracy: 0.8680, Validation Accuracy: 0.8111, Loss: 0.1981\n",
      "Epoch   0 Batch  850/1077 - Train Accuracy: 0.8244, Validation Accuracy: 0.8317, Loss: 0.2231\n",
      "Epoch   0 Batch  860/1077 - Train Accuracy: 0.8545, Validation Accuracy: 0.8409, Loss: 0.1988\n",
      "Epoch   0 Batch  870/1077 - Train Accuracy: 0.8104, Validation Accuracy: 0.8310, Loss: 0.2000\n",
      "Epoch   0 Batch  880/1077 - Train Accuracy: 0.8816, Validation Accuracy: 0.8477, Loss: 0.1839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  890/1077 - Train Accuracy: 0.8862, Validation Accuracy: 0.8335, Loss: 0.1768\n",
      "Epoch   0 Batch  900/1077 - Train Accuracy: 0.8797, Validation Accuracy: 0.8452, Loss: 0.1913\n",
      "Epoch   0 Batch  910/1077 - Train Accuracy: 0.8508, Validation Accuracy: 0.8462, Loss: 0.1723\n",
      "Epoch   0 Batch  920/1077 - Train Accuracy: 0.8562, Validation Accuracy: 0.8608, Loss: 0.1724\n",
      "Epoch   0 Batch  930/1077 - Train Accuracy: 0.8805, Validation Accuracy: 0.8352, Loss: 0.1559\n",
      "Epoch   0 Batch  940/1077 - Train Accuracy: 0.9031, Validation Accuracy: 0.8370, Loss: 0.1486\n",
      "Epoch   0 Batch  950/1077 - Train Accuracy: 0.8836, Validation Accuracy: 0.8693, Loss: 0.1367\n",
      "Epoch   0 Batch  960/1077 - Train Accuracy: 0.8694, Validation Accuracy: 0.8469, Loss: 0.1458\n",
      "Epoch   0 Batch  970/1077 - Train Accuracy: 0.9156, Validation Accuracy: 0.8697, Loss: 0.1571\n",
      "Epoch   0 Batch  980/1077 - Train Accuracy: 0.8828, Validation Accuracy: 0.8778, Loss: 0.1477\n",
      "Epoch   0 Batch  990/1077 - Train Accuracy: 0.8803, Validation Accuracy: 0.8675, Loss: 0.1464\n",
      "Epoch   0 Batch 1000/1077 - Train Accuracy: 0.8951, Validation Accuracy: 0.8857, Loss: 0.1262\n",
      "Epoch   0 Batch 1010/1077 - Train Accuracy: 0.9098, Validation Accuracy: 0.8658, Loss: 0.1249\n",
      "Epoch   0 Batch 1020/1077 - Train Accuracy: 0.9004, Validation Accuracy: 0.8739, Loss: 0.1203\n",
      "Epoch   0 Batch 1030/1077 - Train Accuracy: 0.8988, Validation Accuracy: 0.8512, Loss: 0.1339\n",
      "Epoch   0 Batch 1040/1077 - Train Accuracy: 0.8692, Validation Accuracy: 0.8714, Loss: 0.1306\n",
      "Epoch   0 Batch 1050/1077 - Train Accuracy: 0.9133, Validation Accuracy: 0.8871, Loss: 0.1093\n",
      "Epoch   0 Batch 1060/1077 - Train Accuracy: 0.8922, Validation Accuracy: 0.8327, Loss: 0.1096\n",
      "Epoch   0 Batch 1070/1077 - Train Accuracy: 0.8844, Validation Accuracy: 0.8612, Loss: 0.1186\n",
      "Epoch   1 Batch   10/1077 - Train Accuracy: 0.8968, Validation Accuracy: 0.8686, Loss: 0.1091\n",
      "Epoch   1 Batch   20/1077 - Train Accuracy: 0.9008, Validation Accuracy: 0.8643, Loss: 0.0860\n",
      "Epoch   1 Batch   30/1077 - Train Accuracy: 0.9109, Validation Accuracy: 0.8903, Loss: 0.0952\n",
      "Epoch   1 Batch   40/1077 - Train Accuracy: 0.9258, Validation Accuracy: 0.8775, Loss: 0.0920\n",
      "Epoch   1 Batch   50/1077 - Train Accuracy: 0.9203, Validation Accuracy: 0.8775, Loss: 0.0944\n",
      "Epoch   1 Batch   60/1077 - Train Accuracy: 0.9133, Validation Accuracy: 0.8651, Loss: 0.0902\n",
      "Epoch   1 Batch   70/1077 - Train Accuracy: 0.8873, Validation Accuracy: 0.8842, Loss: 0.0947\n",
      "Epoch   1 Batch   80/1077 - Train Accuracy: 0.9238, Validation Accuracy: 0.8892, Loss: 0.0837\n",
      "Epoch   1 Batch   90/1077 - Train Accuracy: 0.9016, Validation Accuracy: 0.8661, Loss: 0.0977\n",
      "Epoch   1 Batch  100/1077 - Train Accuracy: 0.9156, Validation Accuracy: 0.8871, Loss: 0.0838\n",
      "Epoch   1 Batch  110/1077 - Train Accuracy: 0.9426, Validation Accuracy: 0.8793, Loss: 0.0720\n",
      "Epoch   1 Batch  120/1077 - Train Accuracy: 0.9207, Validation Accuracy: 0.8956, Loss: 0.1017\n",
      "Epoch   1 Batch  130/1077 - Train Accuracy: 0.9137, Validation Accuracy: 0.8622, Loss: 0.0731\n",
      "Epoch   1 Batch  140/1077 - Train Accuracy: 0.9174, Validation Accuracy: 0.8817, Loss: 0.0790\n",
      "Epoch   1 Batch  150/1077 - Train Accuracy: 0.9059, Validation Accuracy: 0.8956, Loss: 0.0859\n",
      "Epoch   1 Batch  160/1077 - Train Accuracy: 0.9207, Validation Accuracy: 0.8878, Loss: 0.0802\n",
      "Epoch   1 Batch  170/1077 - Train Accuracy: 0.9055, Validation Accuracy: 0.8754, Loss: 0.0775\n",
      "Epoch   1 Batch  180/1077 - Train Accuracy: 0.9004, Validation Accuracy: 0.8700, Loss: 0.0727\n",
      "Epoch   1 Batch  190/1077 - Train Accuracy: 0.9367, Validation Accuracy: 0.8903, Loss: 0.0664\n",
      "Epoch   1 Batch  200/1077 - Train Accuracy: 0.8938, Validation Accuracy: 0.9038, Loss: 0.0837\n",
      "Epoch   1 Batch  210/1077 - Train Accuracy: 0.9022, Validation Accuracy: 0.8988, Loss: 0.0798\n",
      "Epoch   1 Batch  220/1077 - Train Accuracy: 0.9235, Validation Accuracy: 0.8920, Loss: 0.0689\n",
      "Epoch   1 Batch  230/1077 - Train Accuracy: 0.9226, Validation Accuracy: 0.8789, Loss: 0.0749\n",
      "Epoch   1 Batch  240/1077 - Train Accuracy: 0.9355, Validation Accuracy: 0.9098, Loss: 0.0658\n",
      "Epoch   1 Batch  250/1077 - Train Accuracy: 0.9300, Validation Accuracy: 0.9077, Loss: 0.0657\n",
      "Epoch   1 Batch  260/1077 - Train Accuracy: 0.9222, Validation Accuracy: 0.8945, Loss: 0.0617\n",
      "Epoch   1 Batch  270/1077 - Train Accuracy: 0.8949, Validation Accuracy: 0.9148, Loss: 0.0763\n",
      "Epoch   1 Batch  280/1077 - Train Accuracy: 0.9066, Validation Accuracy: 0.9183, Loss: 0.0741\n",
      "Epoch   1 Batch  290/1077 - Train Accuracy: 0.9008, Validation Accuracy: 0.9059, Loss: 0.0894\n",
      "Epoch   1 Batch  300/1077 - Train Accuracy: 0.9560, Validation Accuracy: 0.9102, Loss: 0.0579\n",
      "Epoch   1 Batch  310/1077 - Train Accuracy: 0.9281, Validation Accuracy: 0.8789, Loss: 0.0633\n",
      "Epoch   1 Batch  320/1077 - Train Accuracy: 0.9367, Validation Accuracy: 0.8991, Loss: 0.0704\n",
      "Epoch   1 Batch  330/1077 - Train Accuracy: 0.9242, Validation Accuracy: 0.9027, Loss: 0.0701\n",
      "Epoch   1 Batch  340/1077 - Train Accuracy: 0.9502, Validation Accuracy: 0.9183, Loss: 0.0595\n",
      "Epoch   1 Batch  350/1077 - Train Accuracy: 0.9113, Validation Accuracy: 0.8960, Loss: 0.0656\n",
      "Epoch   1 Batch  360/1077 - Train Accuracy: 0.9352, Validation Accuracy: 0.9148, Loss: 0.0534\n",
      "Epoch   1 Batch  370/1077 - Train Accuracy: 0.9297, Validation Accuracy: 0.9183, Loss: 0.0613\n",
      "Epoch   1 Batch  380/1077 - Train Accuracy: 0.9461, Validation Accuracy: 0.9247, Loss: 0.0530\n",
      "Epoch   1 Batch  390/1077 - Train Accuracy: 0.9195, Validation Accuracy: 0.9318, Loss: 0.0666\n",
      "Epoch   1 Batch  400/1077 - Train Accuracy: 0.9238, Validation Accuracy: 0.9183, Loss: 0.0687\n",
      "Epoch   1 Batch  410/1077 - Train Accuracy: 0.9021, Validation Accuracy: 0.9080, Loss: 0.0674\n",
      "Epoch   1 Batch  420/1077 - Train Accuracy: 0.9543, Validation Accuracy: 0.9187, Loss: 0.0468\n",
      "Epoch   1 Batch  430/1077 - Train Accuracy: 0.9246, Validation Accuracy: 0.9254, Loss: 0.0514\n",
      "Epoch   1 Batch  440/1077 - Train Accuracy: 0.9164, Validation Accuracy: 0.9119, Loss: 0.0742\n",
      "Epoch   1 Batch  450/1077 - Train Accuracy: 0.9234, Validation Accuracy: 0.9151, Loss: 0.0526\n",
      "Epoch   1 Batch  460/1077 - Train Accuracy: 0.9320, Validation Accuracy: 0.9052, Loss: 0.0545\n",
      "Epoch   1 Batch  470/1077 - Train Accuracy: 0.9502, Validation Accuracy: 0.9205, Loss: 0.0617\n",
      "Epoch   1 Batch  480/1077 - Train Accuracy: 0.9396, Validation Accuracy: 0.9034, Loss: 0.0510\n",
      "Epoch   1 Batch  490/1077 - Train Accuracy: 0.9367, Validation Accuracy: 0.8874, Loss: 0.0529\n",
      "Epoch   1 Batch  500/1077 - Train Accuracy: 0.9473, Validation Accuracy: 0.9237, Loss: 0.0455\n",
      "Epoch   1 Batch  510/1077 - Train Accuracy: 0.9328, Validation Accuracy: 0.9219, Loss: 0.0523\n",
      "Epoch   1 Batch  520/1077 - Train Accuracy: 0.9695, Validation Accuracy: 0.9180, Loss: 0.0461\n",
      "Epoch   1 Batch  530/1077 - Train Accuracy: 0.9168, Validation Accuracy: 0.9173, Loss: 0.0550\n",
      "Epoch   1 Batch  540/1077 - Train Accuracy: 0.9375, Validation Accuracy: 0.9098, Loss: 0.0424\n",
      "Epoch   1 Batch  550/1077 - Train Accuracy: 0.9012, Validation Accuracy: 0.9261, Loss: 0.0513\n",
      "Epoch   1 Batch  560/1077 - Train Accuracy: 0.9434, Validation Accuracy: 0.9258, Loss: 0.0484\n",
      "Epoch   1 Batch  570/1077 - Train Accuracy: 0.9116, Validation Accuracy: 0.9077, Loss: 0.0705\n",
      "Epoch   1 Batch  580/1077 - Train Accuracy: 0.9453, Validation Accuracy: 0.9425, Loss: 0.0394\n",
      "Epoch   1 Batch  590/1077 - Train Accuracy: 0.9272, Validation Accuracy: 0.9219, Loss: 0.0577\n",
      "Epoch   1 Batch  600/1077 - Train Accuracy: 0.9554, Validation Accuracy: 0.9215, Loss: 0.0519\n",
      "Epoch   1 Batch  610/1077 - Train Accuracy: 0.9507, Validation Accuracy: 0.9506, Loss: 0.0547\n",
      "Epoch   1 Batch  620/1077 - Train Accuracy: 0.9555, Validation Accuracy: 0.9329, Loss: 0.0433\n",
      "Epoch   1 Batch  630/1077 - Train Accuracy: 0.9547, Validation Accuracy: 0.9446, Loss: 0.0410\n",
      "Epoch   1 Batch  640/1077 - Train Accuracy: 0.9546, Validation Accuracy: 0.9254, Loss: 0.0450\n",
      "Epoch   1 Batch  650/1077 - Train Accuracy: 0.9645, Validation Accuracy: 0.9428, Loss: 0.0502\n",
      "Epoch   1 Batch  660/1077 - Train Accuracy: 0.9539, Validation Accuracy: 0.9496, Loss: 0.0421\n",
      "Epoch   1 Batch  670/1077 - Train Accuracy: 0.9492, Validation Accuracy: 0.9308, Loss: 0.0549\n",
      "Epoch   1 Batch  680/1077 - Train Accuracy: 0.9156, Validation Accuracy: 0.9432, Loss: 0.0489\n",
      "Epoch   1 Batch  690/1077 - Train Accuracy: 0.9437, Validation Accuracy: 0.9485, Loss: 0.0467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch  700/1077 - Train Accuracy: 0.9594, Validation Accuracy: 0.9183, Loss: 0.0342\n",
      "Epoch   1 Batch  710/1077 - Train Accuracy: 0.9344, Validation Accuracy: 0.9339, Loss: 0.0363\n",
      "Epoch   1 Batch  720/1077 - Train Accuracy: 0.9198, Validation Accuracy: 0.9272, Loss: 0.0511\n",
      "Epoch   1 Batch  730/1077 - Train Accuracy: 0.9437, Validation Accuracy: 0.9190, Loss: 0.0576\n",
      "Epoch   1 Batch  740/1077 - Train Accuracy: 0.9391, Validation Accuracy: 0.9396, Loss: 0.0393\n",
      "Epoch   1 Batch  750/1077 - Train Accuracy: 0.9293, Validation Accuracy: 0.9272, Loss: 0.0430\n",
      "Epoch   1 Batch  760/1077 - Train Accuracy: 0.9414, Validation Accuracy: 0.9350, Loss: 0.0499\n",
      "Epoch   1 Batch  770/1077 - Train Accuracy: 0.9267, Validation Accuracy: 0.9318, Loss: 0.0434\n",
      "Epoch   1 Batch  780/1077 - Train Accuracy: 0.9184, Validation Accuracy: 0.9162, Loss: 0.0577\n",
      "Epoch   1 Batch  790/1077 - Train Accuracy: 0.8773, Validation Accuracy: 0.9435, Loss: 0.0538\n",
      "Epoch   1 Batch  800/1077 - Train Accuracy: 0.9184, Validation Accuracy: 0.9208, Loss: 0.0420\n",
      "Epoch   1 Batch  810/1077 - Train Accuracy: 0.9621, Validation Accuracy: 0.9492, Loss: 0.0346\n",
      "Epoch   1 Batch  820/1077 - Train Accuracy: 0.9496, Validation Accuracy: 0.9482, Loss: 0.0344\n",
      "Epoch   1 Batch  830/1077 - Train Accuracy: 0.9090, Validation Accuracy: 0.9379, Loss: 0.0444\n",
      "Epoch   1 Batch  840/1077 - Train Accuracy: 0.9711, Validation Accuracy: 0.9379, Loss: 0.0339\n",
      "Epoch   1 Batch  850/1077 - Train Accuracy: 0.9420, Validation Accuracy: 0.9624, Loss: 0.0597\n",
      "Epoch   1 Batch  860/1077 - Train Accuracy: 0.9427, Validation Accuracy: 0.9503, Loss: 0.0425\n",
      "Epoch   1 Batch  870/1077 - Train Accuracy: 0.9326, Validation Accuracy: 0.9442, Loss: 0.0388\n",
      "Epoch   1 Batch  880/1077 - Train Accuracy: 0.9563, Validation Accuracy: 0.9513, Loss: 0.0462\n",
      "Epoch   1 Batch  890/1077 - Train Accuracy: 0.9487, Validation Accuracy: 0.9442, Loss: 0.0383\n",
      "Epoch   1 Batch  900/1077 - Train Accuracy: 0.9605, Validation Accuracy: 0.9556, Loss: 0.0403\n",
      "Epoch   1 Batch  910/1077 - Train Accuracy: 0.9516, Validation Accuracy: 0.9528, Loss: 0.0396\n",
      "Epoch   1 Batch  920/1077 - Train Accuracy: 0.9359, Validation Accuracy: 0.9339, Loss: 0.0354\n",
      "Epoch   1 Batch  930/1077 - Train Accuracy: 0.9598, Validation Accuracy: 0.9350, Loss: 0.0320\n",
      "Epoch   1 Batch  940/1077 - Train Accuracy: 0.9445, Validation Accuracy: 0.9364, Loss: 0.0363\n",
      "Epoch   1 Batch  950/1077 - Train Accuracy: 0.9650, Validation Accuracy: 0.9638, Loss: 0.0368\n",
      "Epoch   1 Batch  960/1077 - Train Accuracy: 0.9568, Validation Accuracy: 0.9588, Loss: 0.0408\n",
      "Epoch   1 Batch  970/1077 - Train Accuracy: 0.9668, Validation Accuracy: 0.9414, Loss: 0.0370\n",
      "Epoch   1 Batch  980/1077 - Train Accuracy: 0.9211, Validation Accuracy: 0.9347, Loss: 0.0449\n",
      "Epoch   1 Batch  990/1077 - Train Accuracy: 0.9638, Validation Accuracy: 0.9485, Loss: 0.0438\n",
      "Epoch   1 Batch 1000/1077 - Train Accuracy: 0.9457, Validation Accuracy: 0.9460, Loss: 0.0355\n",
      "Epoch   1 Batch 1010/1077 - Train Accuracy: 0.9629, Validation Accuracy: 0.9485, Loss: 0.0303\n",
      "Epoch   1 Batch 1020/1077 - Train Accuracy: 0.9762, Validation Accuracy: 0.9425, Loss: 0.0322\n",
      "Epoch   1 Batch 1030/1077 - Train Accuracy: 0.9637, Validation Accuracy: 0.9393, Loss: 0.0326\n",
      "Epoch   1 Batch 1040/1077 - Train Accuracy: 0.9478, Validation Accuracy: 0.9453, Loss: 0.0374\n",
      "Epoch   1 Batch 1050/1077 - Train Accuracy: 0.9645, Validation Accuracy: 0.9627, Loss: 0.0313\n",
      "Epoch   1 Batch 1060/1077 - Train Accuracy: 0.9406, Validation Accuracy: 0.9503, Loss: 0.0335\n",
      "Epoch   1 Batch 1070/1077 - Train Accuracy: 0.9555, Validation Accuracy: 0.9361, Loss: 0.0314\n",
      "Epoch   2 Batch   10/1077 - Train Accuracy: 0.9585, Validation Accuracy: 0.9439, Loss: 0.0333\n",
      "Epoch   2 Batch   20/1077 - Train Accuracy: 0.9520, Validation Accuracy: 0.9567, Loss: 0.0290\n",
      "Epoch   2 Batch   30/1077 - Train Accuracy: 0.9477, Validation Accuracy: 0.9371, Loss: 0.0306\n",
      "Epoch   2 Batch   40/1077 - Train Accuracy: 0.9656, Validation Accuracy: 0.9535, Loss: 0.0269\n",
      "Epoch   2 Batch   50/1077 - Train Accuracy: 0.9500, Validation Accuracy: 0.9425, Loss: 0.0343\n",
      "Epoch   2 Batch   60/1077 - Train Accuracy: 0.9464, Validation Accuracy: 0.9439, Loss: 0.0300\n",
      "Epoch   2 Batch   70/1077 - Train Accuracy: 0.9474, Validation Accuracy: 0.9631, Loss: 0.0317\n",
      "Epoch   2 Batch   80/1077 - Train Accuracy: 0.9484, Validation Accuracy: 0.9442, Loss: 0.0280\n",
      "Epoch   2 Batch   90/1077 - Train Accuracy: 0.9523, Validation Accuracy: 0.9634, Loss: 0.0334\n",
      "Epoch   2 Batch  100/1077 - Train Accuracy: 0.9523, Validation Accuracy: 0.9563, Loss: 0.0311\n",
      "Epoch   2 Batch  110/1077 - Train Accuracy: 0.9668, Validation Accuracy: 0.9492, Loss: 0.0226\n",
      "Epoch   2 Batch  120/1077 - Train Accuracy: 0.9590, Validation Accuracy: 0.9446, Loss: 0.0339\n",
      "Epoch   2 Batch  130/1077 - Train Accuracy: 0.9591, Validation Accuracy: 0.9400, Loss: 0.0289\n",
      "Epoch   2 Batch  140/1077 - Train Accuracy: 0.9667, Validation Accuracy: 0.9407, Loss: 0.0276\n",
      "Epoch   2 Batch  150/1077 - Train Accuracy: 0.9635, Validation Accuracy: 0.9624, Loss: 0.0294\n",
      "Epoch   2 Batch  160/1077 - Train Accuracy: 0.9598, Validation Accuracy: 0.9620, Loss: 0.0286\n",
      "Epoch   2 Batch  170/1077 - Train Accuracy: 0.9578, Validation Accuracy: 0.9698, Loss: 0.0339\n",
      "Epoch   2 Batch  180/1077 - Train Accuracy: 0.9750, Validation Accuracy: 0.9542, Loss: 0.0276\n",
      "Epoch   2 Batch  190/1077 - Train Accuracy: 0.9605, Validation Accuracy: 0.9489, Loss: 0.0234\n",
      "Epoch   2 Batch  200/1077 - Train Accuracy: 0.9613, Validation Accuracy: 0.9528, Loss: 0.0319\n",
      "Epoch   2 Batch  210/1077 - Train Accuracy: 0.9769, Validation Accuracy: 0.9450, Loss: 0.0295\n",
      "Epoch   2 Batch  220/1077 - Train Accuracy: 0.9572, Validation Accuracy: 0.9510, Loss: 0.0326\n",
      "Epoch   2 Batch  230/1077 - Train Accuracy: 0.9606, Validation Accuracy: 0.9542, Loss: 0.0327\n",
      "Epoch   2 Batch  240/1077 - Train Accuracy: 0.9883, Validation Accuracy: 0.9506, Loss: 0.0245\n",
      "Epoch   2 Batch  250/1077 - Train Accuracy: 0.9567, Validation Accuracy: 0.9545, Loss: 0.0301\n",
      "Epoch   2 Batch  260/1077 - Train Accuracy: 0.9669, Validation Accuracy: 0.9581, Loss: 0.0229\n",
      "Epoch   2 Batch  270/1077 - Train Accuracy: 0.9535, Validation Accuracy: 0.9563, Loss: 0.0332\n",
      "Epoch   2 Batch  280/1077 - Train Accuracy: 0.9320, Validation Accuracy: 0.9616, Loss: 0.0311\n",
      "Epoch   2 Batch  290/1077 - Train Accuracy: 0.9555, Validation Accuracy: 0.9542, Loss: 0.0440\n",
      "Epoch   2 Batch  300/1077 - Train Accuracy: 0.9688, Validation Accuracy: 0.9357, Loss: 0.0257\n",
      "Epoch   2 Batch  310/1077 - Train Accuracy: 0.9563, Validation Accuracy: 0.9226, Loss: 0.0269\n",
      "Epoch   2 Batch  320/1077 - Train Accuracy: 0.9609, Validation Accuracy: 0.9489, Loss: 0.0332\n",
      "Epoch   2 Batch  330/1077 - Train Accuracy: 0.9484, Validation Accuracy: 0.9531, Loss: 0.0311\n",
      "Epoch   2 Batch  340/1077 - Train Accuracy: 0.9683, Validation Accuracy: 0.9563, Loss: 0.0270\n",
      "Epoch   2 Batch  350/1077 - Train Accuracy: 0.9605, Validation Accuracy: 0.9492, Loss: 0.0281\n",
      "Epoch   2 Batch  360/1077 - Train Accuracy: 0.9875, Validation Accuracy: 0.9620, Loss: 0.0203\n",
      "Epoch   2 Batch  370/1077 - Train Accuracy: 0.9639, Validation Accuracy: 0.9616, Loss: 0.0269\n",
      "Epoch   2 Batch  380/1077 - Train Accuracy: 0.9512, Validation Accuracy: 0.9602, Loss: 0.0222\n",
      "Epoch   2 Batch  390/1077 - Train Accuracy: 0.9496, Validation Accuracy: 0.9588, Loss: 0.0378\n",
      "Epoch   2 Batch  400/1077 - Train Accuracy: 0.9547, Validation Accuracy: 0.9602, Loss: 0.0368\n",
      "Epoch   2 Batch  410/1077 - Train Accuracy: 0.9280, Validation Accuracy: 0.9648, Loss: 0.0388\n",
      "Epoch   2 Batch  420/1077 - Train Accuracy: 0.9891, Validation Accuracy: 0.9613, Loss: 0.0179\n",
      "Epoch   2 Batch  430/1077 - Train Accuracy: 0.9621, Validation Accuracy: 0.9446, Loss: 0.0239\n",
      "Epoch   2 Batch  440/1077 - Train Accuracy: 0.9437, Validation Accuracy: 0.9627, Loss: 0.0315\n",
      "Epoch   2 Batch  450/1077 - Train Accuracy: 0.9715, Validation Accuracy: 0.9510, Loss: 0.0257\n",
      "Epoch   2 Batch  460/1077 - Train Accuracy: 0.9531, Validation Accuracy: 0.9595, Loss: 0.0282\n",
      "Epoch   2 Batch  470/1077 - Train Accuracy: 0.9675, Validation Accuracy: 0.9528, Loss: 0.0283\n",
      "Epoch   2 Batch  480/1077 - Train Accuracy: 0.9655, Validation Accuracy: 0.9624, Loss: 0.0231\n",
      "Epoch   2 Batch  490/1077 - Train Accuracy: 0.9566, Validation Accuracy: 0.9581, Loss: 0.0294\n",
      "Epoch   2 Batch  500/1077 - Train Accuracy: 0.9723, Validation Accuracy: 0.9556, Loss: 0.0185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch  510/1077 - Train Accuracy: 0.9480, Validation Accuracy: 0.9574, Loss: 0.0290\n",
      "Epoch   2 Batch  520/1077 - Train Accuracy: 0.9810, Validation Accuracy: 0.9666, Loss: 0.0195\n",
      "Epoch   2 Batch  530/1077 - Train Accuracy: 0.9520, Validation Accuracy: 0.9695, Loss: 0.0325\n",
      "Epoch   2 Batch  540/1077 - Train Accuracy: 0.9785, Validation Accuracy: 0.9577, Loss: 0.0224\n",
      "Epoch   2 Batch  550/1077 - Train Accuracy: 0.9383, Validation Accuracy: 0.9542, Loss: 0.0307\n",
      "Epoch   2 Batch  560/1077 - Train Accuracy: 0.9602, Validation Accuracy: 0.9652, Loss: 0.0251\n",
      "Epoch   2 Batch  570/1077 - Train Accuracy: 0.9544, Validation Accuracy: 0.9585, Loss: 0.0337\n",
      "Epoch   2 Batch  580/1077 - Train Accuracy: 0.9680, Validation Accuracy: 0.9567, Loss: 0.0196\n",
      "Epoch   2 Batch  590/1077 - Train Accuracy: 0.9581, Validation Accuracy: 0.9563, Loss: 0.0269\n",
      "Epoch   2 Batch  600/1077 - Train Accuracy: 0.9673, Validation Accuracy: 0.9751, Loss: 0.0275\n",
      "Epoch   2 Batch  610/1077 - Train Accuracy: 0.9679, Validation Accuracy: 0.9606, Loss: 0.0319\n",
      "Epoch   2 Batch  620/1077 - Train Accuracy: 0.9770, Validation Accuracy: 0.9627, Loss: 0.0266\n",
      "Epoch   2 Batch  630/1077 - Train Accuracy: 0.9547, Validation Accuracy: 0.9631, Loss: 0.0271\n",
      "Epoch   2 Batch  640/1077 - Train Accuracy: 0.9736, Validation Accuracy: 0.9638, Loss: 0.0258\n",
      "Epoch   2 Batch  650/1077 - Train Accuracy: 0.9730, Validation Accuracy: 0.9748, Loss: 0.0280\n",
      "Epoch   2 Batch  660/1077 - Train Accuracy: 0.9867, Validation Accuracy: 0.9705, Loss: 0.0211\n",
      "Epoch   2 Batch  670/1077 - Train Accuracy: 0.9734, Validation Accuracy: 0.9585, Loss: 0.0254\n",
      "Epoch   2 Batch  680/1077 - Train Accuracy: 0.9509, Validation Accuracy: 0.9588, Loss: 0.0278\n",
      "Epoch   2 Batch  690/1077 - Train Accuracy: 0.9656, Validation Accuracy: 0.9688, Loss: 0.0251\n",
      "Epoch   2 Batch  700/1077 - Train Accuracy: 0.9727, Validation Accuracy: 0.9673, Loss: 0.0175\n",
      "Epoch   2 Batch  710/1077 - Train Accuracy: 0.9688, Validation Accuracy: 0.9737, Loss: 0.0184\n",
      "Epoch   2 Batch  720/1077 - Train Accuracy: 0.9733, Validation Accuracy: 0.9677, Loss: 0.0262\n",
      "Epoch   2 Batch  730/1077 - Train Accuracy: 0.9645, Validation Accuracy: 0.9577, Loss: 0.0337\n",
      "Epoch   2 Batch  740/1077 - Train Accuracy: 0.9703, Validation Accuracy: 0.9783, Loss: 0.0200\n",
      "Epoch   2 Batch  750/1077 - Train Accuracy: 0.9578, Validation Accuracy: 0.9673, Loss: 0.0201\n",
      "Epoch   2 Batch  760/1077 - Train Accuracy: 0.9672, Validation Accuracy: 0.9620, Loss: 0.0247\n",
      "Epoch   2 Batch  770/1077 - Train Accuracy: 0.9632, Validation Accuracy: 0.9467, Loss: 0.0255\n",
      "Epoch   2 Batch  780/1077 - Train Accuracy: 0.9520, Validation Accuracy: 0.9613, Loss: 0.0337\n",
      "Epoch   2 Batch  790/1077 - Train Accuracy: 0.9305, Validation Accuracy: 0.9602, Loss: 0.0329\n",
      "Epoch   2 Batch  800/1077 - Train Accuracy: 0.9691, Validation Accuracy: 0.9545, Loss: 0.0221\n",
      "Epoch   2 Batch  810/1077 - Train Accuracy: 0.9747, Validation Accuracy: 0.9542, Loss: 0.0168\n",
      "Epoch   2 Batch  820/1077 - Train Accuracy: 0.9754, Validation Accuracy: 0.9673, Loss: 0.0186\n",
      "Epoch   2 Batch  830/1077 - Train Accuracy: 0.9512, Validation Accuracy: 0.9556, Loss: 0.0287\n",
      "Epoch   2 Batch  840/1077 - Train Accuracy: 0.9680, Validation Accuracy: 0.9666, Loss: 0.0232\n",
      "Epoch   2 Batch  850/1077 - Train Accuracy: 0.9684, Validation Accuracy: 0.9606, Loss: 0.0376\n",
      "Epoch   2 Batch  860/1077 - Train Accuracy: 0.9621, Validation Accuracy: 0.9801, Loss: 0.0238\n",
      "Epoch   2 Batch  870/1077 - Train Accuracy: 0.9544, Validation Accuracy: 0.9620, Loss: 0.0218\n",
      "Epoch   2 Batch  880/1077 - Train Accuracy: 0.9746, Validation Accuracy: 0.9656, Loss: 0.0292\n",
      "Epoch   2 Batch  890/1077 - Train Accuracy: 0.9728, Validation Accuracy: 0.9620, Loss: 0.0247\n",
      "Epoch   2 Batch  900/1077 - Train Accuracy: 0.9672, Validation Accuracy: 0.9567, Loss: 0.0243\n",
      "Epoch   2 Batch  910/1077 - Train Accuracy: 0.9617, Validation Accuracy: 0.9606, Loss: 0.0252\n",
      "Epoch   2 Batch  920/1077 - Train Accuracy: 0.9871, Validation Accuracy: 0.9627, Loss: 0.0168\n",
      "Epoch   2 Batch  930/1077 - Train Accuracy: 0.9754, Validation Accuracy: 0.9595, Loss: 0.0161\n",
      "Epoch   2 Batch  940/1077 - Train Accuracy: 0.9684, Validation Accuracy: 0.9556, Loss: 0.0209\n",
      "Epoch   2 Batch  950/1077 - Train Accuracy: 0.9673, Validation Accuracy: 0.9645, Loss: 0.0202\n",
      "Epoch   2 Batch  960/1077 - Train Accuracy: 0.9658, Validation Accuracy: 0.9574, Loss: 0.0227\n",
      "Epoch   2 Batch  970/1077 - Train Accuracy: 0.9809, Validation Accuracy: 0.9528, Loss: 0.0212\n",
      "Epoch   2 Batch  980/1077 - Train Accuracy: 0.9633, Validation Accuracy: 0.9645, Loss: 0.0234\n",
      "Epoch   2 Batch  990/1077 - Train Accuracy: 0.9572, Validation Accuracy: 0.9677, Loss: 0.0262\n",
      "Epoch   2 Batch 1000/1077 - Train Accuracy: 0.9498, Validation Accuracy: 0.9542, Loss: 0.0217\n",
      "Epoch   2 Batch 1010/1077 - Train Accuracy: 0.9680, Validation Accuracy: 0.9563, Loss: 0.0168\n",
      "Epoch   2 Batch 1020/1077 - Train Accuracy: 0.9859, Validation Accuracy: 0.9570, Loss: 0.0200\n",
      "Epoch   2 Batch 1030/1077 - Train Accuracy: 0.9910, Validation Accuracy: 0.9574, Loss: 0.0176\n",
      "Epoch   2 Batch 1040/1077 - Train Accuracy: 0.9815, Validation Accuracy: 0.9563, Loss: 0.0228\n",
      "Epoch   2 Batch 1050/1077 - Train Accuracy: 0.9688, Validation Accuracy: 0.9503, Loss: 0.0204\n",
      "Epoch   2 Batch 1060/1077 - Train Accuracy: 0.9762, Validation Accuracy: 0.9570, Loss: 0.0183\n",
      "Epoch   2 Batch 1070/1077 - Train Accuracy: 0.9734, Validation Accuracy: 0.9467, Loss: 0.0180\n",
      "Epoch   3 Batch   10/1077 - Train Accuracy: 0.9712, Validation Accuracy: 0.9467, Loss: 0.0230\n",
      "Epoch   3 Batch   20/1077 - Train Accuracy: 0.9707, Validation Accuracy: 0.9588, Loss: 0.0193\n",
      "Epoch   3 Batch   30/1077 - Train Accuracy: 0.9859, Validation Accuracy: 0.9688, Loss: 0.0171\n",
      "Epoch   3 Batch   40/1077 - Train Accuracy: 0.9797, Validation Accuracy: 0.9641, Loss: 0.0163\n",
      "Epoch   3 Batch   50/1077 - Train Accuracy: 0.9711, Validation Accuracy: 0.9574, Loss: 0.0213\n",
      "Epoch   3 Batch   60/1077 - Train Accuracy: 0.9706, Validation Accuracy: 0.9663, Loss: 0.0161\n",
      "Epoch   3 Batch   70/1077 - Train Accuracy: 0.9700, Validation Accuracy: 0.9645, Loss: 0.0210\n",
      "Epoch   3 Batch   80/1077 - Train Accuracy: 0.9754, Validation Accuracy: 0.9620, Loss: 0.0168\n",
      "Epoch   3 Batch   90/1077 - Train Accuracy: 0.9594, Validation Accuracy: 0.9709, Loss: 0.0225\n",
      "Epoch   3 Batch  100/1077 - Train Accuracy: 0.9773, Validation Accuracy: 0.9705, Loss: 0.0159\n",
      "Epoch   3 Batch  110/1077 - Train Accuracy: 0.9699, Validation Accuracy: 0.9641, Loss: 0.0151\n",
      "Epoch   3 Batch  120/1077 - Train Accuracy: 0.9688, Validation Accuracy: 0.9627, Loss: 0.0192\n",
      "Epoch   3 Batch  130/1077 - Train Accuracy: 0.9699, Validation Accuracy: 0.9460, Loss: 0.0177\n",
      "Epoch   3 Batch  140/1077 - Train Accuracy: 0.9794, Validation Accuracy: 0.9464, Loss: 0.0178\n",
      "Epoch   3 Batch  150/1077 - Train Accuracy: 0.9699, Validation Accuracy: 0.9652, Loss: 0.0201\n",
      "Epoch   3 Batch  160/1077 - Train Accuracy: 0.9742, Validation Accuracy: 0.9691, Loss: 0.0176\n",
      "Epoch   3 Batch  170/1077 - Train Accuracy: 0.9820, Validation Accuracy: 0.9716, Loss: 0.0209\n",
      "Epoch   3 Batch  180/1077 - Train Accuracy: 0.9809, Validation Accuracy: 0.9684, Loss: 0.0180\n",
      "Epoch   3 Batch  190/1077 - Train Accuracy: 0.9781, Validation Accuracy: 0.9641, Loss: 0.0160\n",
      "Epoch   3 Batch  200/1077 - Train Accuracy: 0.9711, Validation Accuracy: 0.9620, Loss: 0.0217\n",
      "Epoch   3 Batch  210/1077 - Train Accuracy: 0.9773, Validation Accuracy: 0.9755, Loss: 0.0189\n",
      "Epoch   3 Batch  220/1077 - Train Accuracy: 0.9766, Validation Accuracy: 0.9755, Loss: 0.0189\n",
      "Epoch   3 Batch  230/1077 - Train Accuracy: 0.9658, Validation Accuracy: 0.9585, Loss: 0.0205\n",
      "Epoch   3 Batch  240/1077 - Train Accuracy: 0.9902, Validation Accuracy: 0.9528, Loss: 0.0162\n",
      "Epoch   3 Batch  250/1077 - Train Accuracy: 0.9652, Validation Accuracy: 0.9716, Loss: 0.0205\n",
      "Epoch   3 Batch  260/1077 - Train Accuracy: 0.9844, Validation Accuracy: 0.9680, Loss: 0.0172\n",
      "Epoch   3 Batch  270/1077 - Train Accuracy: 0.9730, Validation Accuracy: 0.9638, Loss: 0.0223\n",
      "Epoch   3 Batch  280/1077 - Train Accuracy: 0.9504, Validation Accuracy: 0.9773, Loss: 0.0231\n",
      "Epoch   3 Batch  290/1077 - Train Accuracy: 0.9699, Validation Accuracy: 0.9428, Loss: 0.0315\n",
      "Epoch   3 Batch  300/1077 - Train Accuracy: 0.9708, Validation Accuracy: 0.9574, Loss: 0.0181\n",
      "Epoch   3 Batch  310/1077 - Train Accuracy: 0.9789, Validation Accuracy: 0.9574, Loss: 0.0164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3 Batch  320/1077 - Train Accuracy: 0.9746, Validation Accuracy: 0.9538, Loss: 0.0263\n",
      "Epoch   3 Batch  330/1077 - Train Accuracy: 0.9688, Validation Accuracy: 0.9602, Loss: 0.0198\n",
      "Epoch   3 Batch  340/1077 - Train Accuracy: 0.9889, Validation Accuracy: 0.9691, Loss: 0.0209\n",
      "Epoch   3 Batch  350/1077 - Train Accuracy: 0.9633, Validation Accuracy: 0.9602, Loss: 0.0165\n",
      "Epoch   3 Batch  360/1077 - Train Accuracy: 0.9805, Validation Accuracy: 0.9680, Loss: 0.0125\n",
      "Epoch   3 Batch  370/1077 - Train Accuracy: 0.9717, Validation Accuracy: 0.9521, Loss: 0.0183\n",
      "Epoch   3 Batch  380/1077 - Train Accuracy: 0.9664, Validation Accuracy: 0.9670, Loss: 0.0138\n",
      "Epoch   3 Batch  390/1077 - Train Accuracy: 0.9863, Validation Accuracy: 0.9723, Loss: 0.0228\n",
      "Epoch   3 Batch  400/1077 - Train Accuracy: 0.9727, Validation Accuracy: 0.9716, Loss: 0.0259\n",
      "Epoch   3 Batch  410/1077 - Train Accuracy: 0.9391, Validation Accuracy: 0.9670, Loss: 0.0335\n",
      "Epoch   3 Batch  420/1077 - Train Accuracy: 0.9910, Validation Accuracy: 0.9815, Loss: 0.0115\n",
      "Epoch   3 Batch  430/1077 - Train Accuracy: 0.9738, Validation Accuracy: 0.9680, Loss: 0.0153\n",
      "Epoch   3 Batch  440/1077 - Train Accuracy: 0.9605, Validation Accuracy: 0.9574, Loss: 0.0218\n",
      "Epoch   3 Batch  450/1077 - Train Accuracy: 0.9789, Validation Accuracy: 0.9602, Loss: 0.0190\n",
      "Epoch   3 Batch  460/1077 - Train Accuracy: 0.9672, Validation Accuracy: 0.9560, Loss: 0.0203\n",
      "Epoch   3 Batch  470/1077 - Train Accuracy: 0.9790, Validation Accuracy: 0.9663, Loss: 0.0199\n",
      "Epoch   3 Batch  480/1077 - Train Accuracy: 0.9819, Validation Accuracy: 0.9698, Loss: 0.0152\n",
      "Epoch   3 Batch  490/1077 - Train Accuracy: 0.9668, Validation Accuracy: 0.9560, Loss: 0.0213\n",
      "Epoch   3 Batch  500/1077 - Train Accuracy: 0.9676, Validation Accuracy: 0.9698, Loss: 0.0122\n",
      "Epoch   3 Batch  510/1077 - Train Accuracy: 0.9684, Validation Accuracy: 0.9801, Loss: 0.0181\n",
      "Epoch   3 Batch  520/1077 - Train Accuracy: 0.9896, Validation Accuracy: 0.9744, Loss: 0.0131\n",
      "Epoch   3 Batch  530/1077 - Train Accuracy: 0.9734, Validation Accuracy: 0.9592, Loss: 0.0223\n",
      "Epoch   3 Batch  540/1077 - Train Accuracy: 0.9809, Validation Accuracy: 0.9503, Loss: 0.0147\n",
      "Epoch   3 Batch  550/1077 - Train Accuracy: 0.9574, Validation Accuracy: 0.9581, Loss: 0.0179\n",
      "Epoch   3 Batch  560/1077 - Train Accuracy: 0.9730, Validation Accuracy: 0.9645, Loss: 0.0164\n",
      "Epoch   3 Batch  570/1077 - Train Accuracy: 0.9766, Validation Accuracy: 0.9801, Loss: 0.0218\n",
      "Epoch   3 Batch  580/1077 - Train Accuracy: 0.9911, Validation Accuracy: 0.9592, Loss: 0.0112\n",
      "Epoch   3 Batch  590/1077 - Train Accuracy: 0.9581, Validation Accuracy: 0.9567, Loss: 0.0212\n",
      "Epoch   3 Batch  600/1077 - Train Accuracy: 0.9699, Validation Accuracy: 0.9585, Loss: 0.0213\n",
      "Epoch   3 Batch  610/1077 - Train Accuracy: 0.9782, Validation Accuracy: 0.9812, Loss: 0.0208\n",
      "Epoch   3 Batch  620/1077 - Train Accuracy: 0.9762, Validation Accuracy: 0.9656, Loss: 0.0175\n",
      "Epoch   3 Batch  630/1077 - Train Accuracy: 0.9793, Validation Accuracy: 0.9744, Loss: 0.0157\n",
      "Epoch   3 Batch  640/1077 - Train Accuracy: 0.9810, Validation Accuracy: 0.9727, Loss: 0.0170\n",
      "Epoch   3 Batch  650/1077 - Train Accuracy: 0.9777, Validation Accuracy: 0.9801, Loss: 0.0193\n",
      "Epoch   3 Batch  660/1077 - Train Accuracy: 0.9879, Validation Accuracy: 0.9659, Loss: 0.0179\n",
      "Epoch   3 Batch  670/1077 - Train Accuracy: 0.9851, Validation Accuracy: 0.9638, Loss: 0.0158\n",
      "Epoch   3 Batch  680/1077 - Train Accuracy: 0.9632, Validation Accuracy: 0.9638, Loss: 0.0178\n",
      "Epoch   3 Batch  690/1077 - Train Accuracy: 0.9742, Validation Accuracy: 0.9684, Loss: 0.0195\n",
      "Epoch   3 Batch  700/1077 - Train Accuracy: 0.9883, Validation Accuracy: 0.9592, Loss: 0.0117\n",
      "Epoch   3 Batch  710/1077 - Train Accuracy: 0.9926, Validation Accuracy: 0.9819, Loss: 0.0108\n",
      "Epoch   3 Batch  720/1077 - Train Accuracy: 0.9910, Validation Accuracy: 0.9688, Loss: 0.0167\n",
      "Epoch   3 Batch  730/1077 - Train Accuracy: 0.9629, Validation Accuracy: 0.9673, Loss: 0.0244\n",
      "Epoch   3 Batch  740/1077 - Train Accuracy: 0.9766, Validation Accuracy: 0.9709, Loss: 0.0161\n",
      "Epoch   3 Batch  750/1077 - Train Accuracy: 0.9680, Validation Accuracy: 0.9695, Loss: 0.0181\n",
      "Epoch   3 Batch  760/1077 - Train Accuracy: 0.9852, Validation Accuracy: 0.9659, Loss: 0.0142\n",
      "Epoch   3 Batch  770/1077 - Train Accuracy: 0.9654, Validation Accuracy: 0.9730, Loss: 0.0212\n",
      "Epoch   3 Batch  780/1077 - Train Accuracy: 0.9574, Validation Accuracy: 0.9656, Loss: 0.0254\n",
      "Epoch   3 Batch  790/1077 - Train Accuracy: 0.9703, Validation Accuracy: 0.9805, Loss: 0.0245\n",
      "Epoch   3 Batch  800/1077 - Train Accuracy: 0.9652, Validation Accuracy: 0.9499, Loss: 0.0165\n",
      "Epoch   3 Batch  810/1077 - Train Accuracy: 0.9717, Validation Accuracy: 0.9613, Loss: 0.0139\n",
      "Epoch   3 Batch  820/1077 - Train Accuracy: 0.9934, Validation Accuracy: 0.9631, Loss: 0.0131\n",
      "Epoch   3 Batch  830/1077 - Train Accuracy: 0.9734, Validation Accuracy: 0.9616, Loss: 0.0233\n",
      "Epoch   3 Batch  840/1077 - Train Accuracy: 0.9832, Validation Accuracy: 0.9709, Loss: 0.0175\n",
      "Epoch   3 Batch  850/1077 - Train Accuracy: 0.9676, Validation Accuracy: 0.9812, Loss: 0.0276\n",
      "Epoch   3 Batch  860/1077 - Train Accuracy: 0.9754, Validation Accuracy: 0.9759, Loss: 0.0151\n",
      "Epoch   3 Batch  870/1077 - Train Accuracy: 0.9737, Validation Accuracy: 0.9787, Loss: 0.0135\n",
      "Epoch   3 Batch  880/1077 - Train Accuracy: 0.9867, Validation Accuracy: 0.9762, Loss: 0.0213\n",
      "Epoch   3 Batch  890/1077 - Train Accuracy: 0.9807, Validation Accuracy: 0.9719, Loss: 0.0174\n",
      "Epoch   3 Batch  900/1077 - Train Accuracy: 0.9820, Validation Accuracy: 0.9641, Loss: 0.0164\n",
      "Epoch   3 Batch  910/1077 - Train Accuracy: 0.9658, Validation Accuracy: 0.9517, Loss: 0.0163\n",
      "Epoch   3 Batch  920/1077 - Train Accuracy: 0.9867, Validation Accuracy: 0.9783, Loss: 0.0111\n",
      "Epoch   3 Batch  930/1077 - Train Accuracy: 0.9840, Validation Accuracy: 0.9670, Loss: 0.0124\n",
      "Epoch   3 Batch  940/1077 - Train Accuracy: 0.9688, Validation Accuracy: 0.9599, Loss: 0.0171\n",
      "Epoch   3 Batch  950/1077 - Train Accuracy: 0.9803, Validation Accuracy: 0.9549, Loss: 0.0135\n",
      "Epoch   3 Batch  960/1077 - Train Accuracy: 0.9732, Validation Accuracy: 0.9645, Loss: 0.0158\n",
      "Epoch   3 Batch  970/1077 - Train Accuracy: 0.9770, Validation Accuracy: 0.9599, Loss: 0.0159\n",
      "Epoch   3 Batch  980/1077 - Train Accuracy: 0.9770, Validation Accuracy: 0.9592, Loss: 0.0174\n",
      "Epoch   3 Batch  990/1077 - Train Accuracy: 0.9807, Validation Accuracy: 0.9677, Loss: 0.0156\n",
      "Epoch   3 Batch 1000/1077 - Train Accuracy: 0.9661, Validation Accuracy: 0.9585, Loss: 0.0155\n",
      "Epoch   3 Batch 1010/1077 - Train Accuracy: 0.9820, Validation Accuracy: 0.9705, Loss: 0.0115\n",
      "Epoch   3 Batch 1020/1077 - Train Accuracy: 0.9883, Validation Accuracy: 0.9819, Loss: 0.0143\n",
      "Epoch   3 Batch 1030/1077 - Train Accuracy: 0.9953, Validation Accuracy: 0.9741, Loss: 0.0119\n",
      "Epoch   3 Batch 1040/1077 - Train Accuracy: 0.9716, Validation Accuracy: 0.9485, Loss: 0.0188\n",
      "Epoch   3 Batch 1050/1077 - Train Accuracy: 0.9895, Validation Accuracy: 0.9506, Loss: 0.0116\n",
      "Epoch   3 Batch 1060/1077 - Train Accuracy: 0.9820, Validation Accuracy: 0.9727, Loss: 0.0134\n",
      "Epoch   3 Batch 1070/1077 - Train Accuracy: 0.9816, Validation Accuracy: 0.9620, Loss: 0.0125\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     source_sequence_length: sources_lengths,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     source_sequence_length: valid_sources_lengths,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "helper.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()\n",
    "load_path = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence\n",
    "Implemented the function `sentence_to_seq()` to preprocess new sentences.\n",
    "To-dos :p\n",
    "\n",
    "- Convert the sentence to lowercase\n",
    "- Convert words into ids using `vocab_to_int`\n",
    " - Convert words not in the vocabulary, to the `<UNK>` word id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "  \n",
    "    # TODO: Implement Function\n",
    "    sentence_int = list()\n",
    "    \n",
    "    for word in sentence.lower().split(' '):\n",
    "        try:\n",
    "            word_int = vocab_to_int[word]\n",
    "        except:\n",
    "            word_int = vocab_to_int['<UNK>']\n",
    "        sentence_int.append(word_int)\n",
    "\n",
    "    return sentence_int\n",
    "\n",
    "\n",
    "tests.test_sentence_to_seq(sentence_to_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation...yosh!\n",
    "This will translate `translate_sentence` from English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [209, 204, 124, 85, 34, 213, 17]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [80, 49, 69, 183, 141, 312, 284, 190, 1]\n",
      "  French Words: il a vu un vieux camion jaune . <EOS>\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_sequence_length: [len(translate_sentence)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imperfect Translation\n",
    "I noticed that some sentences translated better than others.  The reason was since the dataset I've used only has a vocabulary of 227 English words of the thousands that we use, I'll only see good results using these words.  For this project, I didn't need a perfect translation but a learning experience. However, if we want to create a better translation model, we'll need better data and a better GPU and days of training-phewww.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
